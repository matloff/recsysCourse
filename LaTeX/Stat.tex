\chapter{Statistical Models}  
\label{chap:mixed} 

Let $Y_{ij}$ denote the rating user $i$ gives item $j$.   As before,
most of these values are unknown, and thus we wish to predict them.
That latter point suggests regression modeling, which we will do in this
chapter.  We'll use both the classical linear and logistic models, as
well as machine learning methods.

\section{Latent Factor Models} 

The most basic model here is

\begin{equation}
Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij},~ i = 1,...,m;~ j =
1,...,N_{i.}
\end{equation}

where $m$ is the number of users and $N_{i.}$ is the number of ratings
we have for user $i$.  (Define $N_{.j}$ to be the number of users rating
item $j$.) Other than $\mu$, all terms have expected value 0.  The terms
are interpreted as follows:

\begin{itemize}

\item $\mu$:  overall mean rating of all items, past, present and future

\item $\mu + \alpha_i$: overall mean rating of all items, past, present
and future, by user $i$; $\alpha_i$ then reflects a tendency for user
$i$ to give more liberal ratings ($\alpha_i >0$) or harsher ratings
($\alpha_i < 0$)

\item $\mu + \beta_j$: overall mean rating of item $j$ by all users,
past, present and future; $\beta_j$ then reflects a tendency for item
$j$ to receive more liberal ratings ($\beta_j >0$) or harsher ratings
($\beta_j < 0$)

\item $\epsilon_{ij}$ is a ``miscellaneous'' term that account for
all factors not accounted for in the model

\end{itemize} 

The $\alpha_i$ and $\beta_j$ terms are called \textit{latent} factors, a
term meaning ``hidden.''  We can't observe user, say, 29's tendency to give
high or low ratings; it's just a model for some kind of driving force in
that user's behavior.  The ``synthetic, representative'' users in
Chapter \ref{chap:svd} are also called \textit{latent} in the RS
literature.

The analogous model for logistic regression, i.e.\ binary $Y$, is

\begin{equation}
P(Y_{ij} = 1 ~|~ i,j) = 
\frac{1}{1 + \exp[-(\mu + \alpha_i + \beta_j)]}
\end{equation}

\subsection{Estimating the Latent Factors}

To predict the unobserved $Y_{ij}$, we will need to estimate $\mu$ and
the $\alpha_i$ and $\beta_j$.  Let's see how.

\subsubsection{Linear Case}

Intuitively, estimators in the linear case are simply sample analogs of
the corresponding population quantities:

\begin{equation}
\widehat{\mu} = \frac{1}{N} \sum_{i,j \textrm{ obs.}} Y_{ij}
\end{equation}

\begin{equation}
\label{yi.}
\widehat{\mu} + \widehat{\alpha}_i = \frac{1}{N_{i.}}
\sum_{j, \textrm{ obs.}}^{N_{i.}} Y_{ij}
\end{equation}

\begin{equation}
\label{y.j}
\widehat{\mu} + \widehat{\beta}_j = \frac{1}{N_{.j}}
\sum_{i, \textrm{ obs.}}^{N_{.j}} Y_{ij}
\end{equation}

where $N = \sum_{i} N_{i.}$ and ``obs.'' means summation over all
observed values, i.e.\ not the missing ones.  So for example 

\begin{equation}
\widehat{\alpha}_i = \frac{1}{N_{i.}} 
\sum_{j, \textrm{ obs.}}^{N_{i.}} Y_{ij}
- \frac{1}{N} \sum_{i,j \textrm{ obs.}} Y_{ij}
\end{equation}

The reader may recognize the latter two quantities as the biases from
the preceding chapter.

\subsubsection{Logistic Case}

The approach of the last section, a special case of a technique called
the \textit{Method of Moments}, would not work as well in the logistic
case.  For example, consider (\ref{yi.}), which would now be

\begin{equation}
\label{nonlin1}
\widehat{\mu} + \widehat{\alpha}_i = 
\frac{1}{N_{i.}} \sum_{j=1}^{N_{i.}}
\frac{1}{1 + \exp[-(\widehat{\mu} + \widehat{\alpha_i} + \widehat{\beta_j})]}
\end{equation}

Now we'd have a set of nonlinear equations to solve.

Instead, what people use is the \textit{Method of Maximum Likelihood}.
To be sure, we still have a set of nonlinear equations to solve, but MLE
has certain optimal statistical properties, and the \textbf{lme4}
package (included in \textbf{rectools}) uses MLE, so we'll go that
route.\footnote{The \textbf{mle} package is for a general class of
methods known as \textit{mixed models}, not specifically for RS.}

To see how MLE works, consider the following game.  I toss a coin until
I accumulate $r$ heads.  I don't tell you what value I've chosen for
$r$, but I do tell you $K$, the number of tosses I needed.  You then
must guess the value of $r$.  Well, elementary probability calculations 
show that\footnote{This is a family of distributions known as
\textit{negative binomial.}}

\begin{equation}
\label{negbin}
P(K = u) = \binom{u-1}{r-1} 0.5^u,~ u = r, r+1, ...
\end{equation}

Say I tell you $K = 7$.  Then what you might do is find the value of $r$
that maximizes 

\begin{equation}
\label{negbin7}
\binom{6}{r-1} 0.5^5
\end{equation}

You are asking, ``What value of $r$ would have made our data ($K = 5$)
most likely?''  Trying $r = 1,2,...,5$, one finds that $r = 4$ maximizes
(\ref{negbin7}), so we would take that as our guess.

Now back to the RS estimation.  It is assumed that the $\alpha_i$ and
$\beta_j$ are independent, normally distributed random variables.
The joint likelihood of all the observed $Y_{ij}$ is then quite complex,
but it can be maximized, which is what \textbf{lme4} does for
\textbf{rectools}.  

\subsection{Incorporation of Covariate Data}

Instead of the old model, 

\begin{equation}
Y = \mu + \alpha + \beta + \epsilon
\end{equation}

we now have

\begin{equation}
\label{modelwithcovs}
Y = X' \gamma + \alpha + \beta + \epsilon
\end{equation}

where $X$ is the vector of covariates and $\gamma$ is the vector of
linear regression coefficients.  Note that the old $\mu$ term is now
absent, at least explicitly.  It now has become $\gamma_0$, and $X$ has
a 1 component.

\subsection{Estimation Approach}

Write the data version of the model (\ref{modelwithcovs}), with
estimates rather than population values, as 

\begin{equation}
Y_{ij} = X_{ij}' \widehat{\gamma} +
\widehat{\alpha}_i + \widehat{\beta_j} + \widehat{\epsilon}_{ij}
\end{equation}

Now, holding $i$ fixed, average both sides over $j$.  The left side will
be (\ref{yi.}).  Since $E\beta = E\epsilon = 0$ and using independence,
the average of the last two terms will be about 0.  So, the average of
the right side will be approximately

\begin{equation}
\overline{X}_{i.}' \widehat{\gamma} +
\widehat{\alpha}_{i} 
\end{equation}

where the vector $\overline{X}_{i.}$ is the average value of $X_{ij}$.

Setting this to (\ref{yi.}), we have

\begin{equation}
\widehat{\alpha}_i = \frac{1}{N_{i.}}
\sum_{j, \textrm{ obs.}}^{N_{i.}} Y_{ij} -
\overline{X}_{i.}' \widehat{\gamma} 
\end{equation}

Similarly,

\begin{equation}
\widehat{\beta}_j = \frac{1}{N_{.j}}
\sum_{i, \textrm{ obs.}}^{N_{.j}} Y_{ij} -
\overline{X}_{.j}' \widehat{\gamma} 
\end{equation}

\subsection{Example:  InstEval Data}

\begin{lstlisting}
> getInstEval()
> mmout <- trainMM(ivl)
\end{lstlisting}

Let's browse:

\begin{lstlisting}
> names(mmout)
[1] "alphai" "betaj"  "lmout"  "Ni."    "N.j" 
> mmout$alphai[4]
     1000 
-1.095866 
> mmout$alphai[5]
    1001 
0.892603 
\end{lstlisting}

So user 1000 seems to give harsher ratings than average, while user 1001
is more liberal in her ratings.\footnote{The reader may wonder why user
1000 would show up fourth in this vector of $\widehat{\alpha}_i$.  The
reason is that the code indexes users by the character form of their
IDs.}

The above used the covariates.  Here is the no-covariates version:

\begin{lstlisting}
mmout.nocov <- trainMM(ivl[,1:3])
\end{lstlisting}

Let's predict the rating user 8 would give instructor 72:\footnote{As an
exercise, the reader should check that our observed data does not
contain this rating, but that instructor 72 is in the data.}

\begin{lstlisting}
> predict(mmout.nocov,matrix(c(8,72),nrow=1))
[1] 2.912437
\end{lstlisting}

\subsection{Implementation}

Below are excerpts of the code from \textbf{trainMM()}.  First, we find
(\ref{yi.}) and (\ref{y.j}):

\begin{lstlisting}
haveCovs <- ncol(ratingsIn) > 3
n <- nrow(ratingsIn)
userRowGrps <- split(1:n, users)
itemRowGrps <- split(1:n, items)
uimean <- function(uirowgrp) mean(ratings[uirowgrp])
Yi. <- sapply(userRowGrps, uimean)
Y.j <- sapply(itemRowGrps, uimean)
\end{lstlisting}

Now for the covariates case:

\begin{lstlisting}
cmd <- paste("lmout <- lm(", nms[3], " ~ .,data=ratingsIn[,-(1:2)])",
    sep = "")
eval(parse(text = cmd))
xmeans <- function(uirowgrp) colMeans(ratingsIn[uirowgrp,
    -(1:3), drop = FALSE])
Xi. <- t(sapply(userRowGrps, xmeans))
X.j <- t(sapply(itemRowGrps, xmeans))
predsa <- predict(lmout, as.data.frame(Xi.))
predsb <- predict(lmout, as.data.frame(X.j))
...
alphai <- Yi. - predsa
betaj <- Y.j - predsb
\end{lstlisting}

Since we want to make use of the original variable names, we built up
the \textbf{lm()} call as a character string, then execute that string.

\section{Full Regression Models}

As will be seen shortly, it is natural to consider regression models for
the $Y_{ij}$.  To illustrate this, let's again consider the InstEval
data.

\subsection{Linear, Logistic}

In the original form of the InstEval data, i.e.\ not the one returned
from the \textbf{rectools} function \textbf{getInstEval()}, the first
two columns, which are the user and item IDs, are R factors.  Assuming
for now that we are not using covariates, we could
then make a call like

\begin{lstlisting}
lm(y ~ s + d, data=InstEval[,c(1,2,7]) 
\end{lstlisting}

Recall that R automatically converts factor predictors to dummy
variables, in this case 2972 of them for user IDs and 1128 for item IDs.
We could then use the return value in \textbf{predict()} calls.
The case of binary $Y$ and a logistic model would be handled similarly.
If we do use covariates, simply specify \textbf{data=InstEval} above.

This will lead to long run times, since we will have so many predictors,
in this case 2972+1128 = 4100 of them.  Indeed, that is why \textbf{lme4}
has a long run time.

However, the use of dummies in the linear case is essentially the same
as our Method of Moments approach earlier in this chapter.  This
suggests a better way:  Take our $Yi.$ and $Y.j$
as our new data.

For instance, here is row 51 of InstEval (for compactness,
without covariates):

\begin{lstlisting}
   s    d y 
51 8 1884 5       
\end{lstlisting}

The value of (\ref{yi.}) for user 8 is

\begin{lstlisting}
> tapply(InstEval$y,InstEval$s,mean)['8']
  8 
3.8 
\end{lstlisting}

while (\ref{y.j}) is

\begin{lstlisting}
> tapply(InstEval$y,InstEval$d,mean)['1884']
    1884 
3.141176 
\end{lstlisting}

We could then replace row 51 by

\begin{lstlisting}
   s    d   y 
51 3.14 3.8 5       
\end{lstlisting}

We then call \textbf{lm()} on the new data.

The \textbf{rectools} functions \textbf{getUINN()} and
\textbf{convertX()} perform this transformation.

\subsection{Machine Learning}
