\chapter{Statistical Models}  
\label{chap:mixed} 

Let $Y_{ij}$ denote the rating user $i$ gives item $j$.   As before,
most of these values are unknown, and thus we wish to predict them.
That latter point suggests regression modeling, which we will do in this
chapter.  We'll use both the classical linear and logistic models, as
well as machine learning methods.

\section{Latent Factor Models} 

The most basic model here is

\begin{equation}
Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij},~ i = 1,...,m;~ j =
1,...,N_{i.}
\end{equation}

where $m$ is the number of users and $N_{i.}$ is the number of ratings
we have for user $i$.  (Define $N_{.j}$ to be the number of users rating
item $j$.) Other than $\mu$, all terms have expected value 0.  The terms
are interpreted as follows:

\begin{itemize}

\item $\mu$:  overall mean rating of all items, past, present and future

\item $\mu + \alpha_i$: overall mean rating of all items, past, present
and future, by user $i$; $\alpha_i$ then reflects a tendency for user
$i$ to give more liberal ratings ($\alpha_i >0$) or harsher ratings
($\alpha_i < 0$)

\item $\mu + \beta_j$: overall mean rating of item $j$ by all users,
past, present and future; $\beta_j$ then reflects a tendency for item
$j$ to receive more liberal ratings ($\beta_j >0$) or harsher ratings
($\beta_j < 0$)

\item $\epsilon_{ij}$ is a ``miscellaneous'' term that account for
all factors not accounted for in the model

\end{itemize} 

The $\alpha_i$ and $\beta_j$ terms are called \textit{latent} factors, a
term meaning ``hidden.''  We can't observe user, say, 29's tendency to give
high or low ratings; it's just a model for some kind of driving force in
that user's behavior.  The ``synthetic, representative'' users in
Chapter \ref{chap:svd} are also called \textit{latent} in the RS
literature.

The analogous model for logistic regression, i.e.\ binary $Y$, is

\begin{equation}
P(Y_{ij} = 1 ~|~ i,j) = 
\frac{1}{1 + \exp[-(\mu + \alpha_i + \beta_j)]}
\end{equation}

\subsection{Estimating the Latent Factors}

To predict the unobserved $Y_{ij}$, we will need to estimate $\mu$ and
the $\alpha_i$ and $\beta_j$.  Let's see how.

\subsubsection{Linear Case}

Intuitively, estimators in the linear case are simply sample analogs of
the corresponding population quantities:

\begin{equation}
Y_{..} = \widehat{\mu} = \frac{1}{N} \sum_{i,j \textrm{ obs.}} Y_{ij}
\end{equation}

\begin{equation}
\label{yi.}
Y_{i.} = \widehat{\mu} + \widehat{\alpha}_i = \frac{1}{N_{i.}}
\sum_{j, \textrm{ obs.}} Y_{ij}
\end{equation}

\begin{equation}
\label{y.j}
Y_{.j} = \widehat{\mu} + \widehat{\beta}_j = \frac{1}{N_{.j}}
\sum_{i, \textrm{ obs.}} Y_{ij}
\end{equation}

where $N = \sum_{i} N_{i.}$ and ``obs.'' means summation over all
observed values, i.e.\ not the missing ones.  So 

\begin{equation}
\widehat{\alpha}_i = Y_{i.}-Y_{..},~ 
\widehat{\beta}_j = Y_{.j}-Y_{..}
% \frac{1}{N_{i.}} 
% \sum_{j, \textrm{ obs.}}^{N_{i.}} Y_{ij}
% - \frac{1}{N} \sum_{i,j \textrm{ obs.}} Y_{ij}
\end{equation}

The reader may recognize the latter two quantities as the biases from
the preceding chapter (without $Y_{..}$).

\subsubsection{Logistic Case}

The approach of the last section, a special case of a technique called
the \textit{Method of Moments}, would not work as well in the logistic
case.  For example, consider (\ref{yi.}), which would now be, for each
$i$,

\begin{equation}
\label{nonlin1}
\widehat{\mu} + \widehat{\alpha}_i = 
\frac{1}{N_{i.}} \sum_{j=1, \textrm{ obs.}}
\frac{1}{1 + \exp[-(\widehat{\mu} + \widehat{\alpha_i} + \widehat{\beta_j})]}
\end{equation}

We have a similar set of equations (as $j$ varies) corresponding to
items.  We'd now have a set of nonlinear equations to solve.

Instead, what people use is the \textit{Method of Maximum Likelihood}.
To be sure, we still have a set of nonlinear equations to solve, but MLE
has certain optimal statistical properties, and the \textbf{lme4}
package (included in \textbf{rectools}) uses MLE, so we'll go that
route.\footnote{The \textbf{mle} package is for a general class of
methods known as \textit{mixed models}, not specifically for RS.}

To see how MLE works, consider the following game.  I toss a coin until
I accumulate $r$ heads.  I don't tell you what value I've chosen for
$r$, but I do tell you $K$, the number of tosses I needed.  You then
must guess the value of $r$.  Well, elementary probability calculations 
show that\footnote{This is a family of distributions known as
\textit{negative binomial.}}

\begin{equation}
\label{negbin}
P(K = u) = \binom{u-1}{r-1} 0.5^u,~ u = r, r+1, ...
\end{equation}

Say I tell you $K = 7$.  Then what you might do is find the value of $r$
that maximizes 

\begin{equation}
\label{negbin7}
\binom{6}{r-1} 0.5^5
\end{equation}

You are asking, ``What value of $r$ would have made our data ($K = 7$)
most likely?''  Trying $r = 1,2,...,5$, one finds that $r = 4$ maximizes
(\ref{negbin7}), so we would take that as our guess.\footnote{By the
way, here is how the Method of Moments approach would work here. For the
negative binomial distribution it is known that $E(K) = r/p$, where $p$
is the probability of ``success,'' in this setting meaning heads.  So
$E(K) = 2r$.  Under MM, we would set $\overline{K} = 2 \widehat{r}$,
where the left-hand side is the average of all values of $K$ in our
data.  We only did the ``experiment'' once, so $\overline{K} = 6$ and we
guess $r$ to be 3.5.  Of course, $r$ must be an integer, so we'd guess 3
o 4.}

Now back to the RS estimation.  It is assumed that the $\alpha_i$ and
$\beta_j$ are independent, normally distributed random variables.  The
joint likelihood of all the observed $Y_{ij}$ (now incorporating both
probabilities and probability density function values) is then quite
complex, but it can be maximized, which is what \textbf{lme4} does for
\textbf{rectools}.  

\subsection{Incorporation of Covariate Data}

Instead of the old model, 

\begin{equation}
Y = \mu + \alpha + \beta + \epsilon
\end{equation}

we now have

\begin{equation}
\label{modelwithcovs}
Y = X' \gamma + \alpha + \beta + \epsilon
\end{equation}

where $X$ is the vector of covariates and $\gamma$ is the vector of
linear regression coefficients.  Note that the old $\mu$ term is now
absent, at least explicitly.  It now has become $\gamma_0$, and $X$ has
a 1 component.  So for instance $X$ may be (1,age,gender).

\subsection{Estimation Approach}

Write the data version of the model (\ref{modelwithcovs}), with
estimates rather than population values, as 

\begin{equation}
Y_{ij} = X_{ij}' \widehat{\gamma} +
\widehat{\alpha}_i + \widehat{\beta_j} + \widehat{\epsilon}_{ij}
\end{equation}

Now, holding $i$ fixed, average both sides over $j$.  The left side will
be $Y_{i.}$.  Since $E\beta = E\epsilon = 0$ and we assume independence,
the average of the last two terms will be about 0.  So, the average of
the right side will be approximately

\begin{equation}
\overline{X}_{i.}' \widehat{\gamma} +
\widehat{\alpha}_{i} 
\end{equation}

where the vector $\overline{X}_{i.}$ is the average value of the vectors
$X_{ij}$ over $j$.

Setting this to (\ref{yi.}), we have

\begin{equation}
\widehat{\alpha}_i = 
Y_{i.} -
\overline{X}_{i.}' \widehat{\gamma} 
\end{equation}

Similarly,

\begin{equation}
\widehat{\beta}_j = 
Y_{.j} -
\overline{X}_{i.}' \widehat{\gamma} 
\end{equation}

\subsection{Example:  InstEval Data}

\begin{lstlisting}
> getInstEval()
> mmout <- trainMM(ivl)
\end{lstlisting}

Let's browse:

\begin{lstlisting}
> names(mmout)
[1] "alphai" "betaj"  "lmout"  "Ni."    "N.j" 
> mmout$alphai[4]
     1000 
-1.095866 
> mmout$alphai[5]
    1001 
0.892603 
\end{lstlisting}

So user 1000 seems to give harsher ratings than average, while user 1001
is more liberal in her ratings.\footnote{The reader may wonder why user
1000 would show up fourth in this vector of $\widehat{\alpha}_i$.  The
reason is that the code indexes users by the character form of their
IDs.}

The above used the covariates.  Here is the no-covariates version:

\begin{lstlisting}
mmout.nocov <- trainMM(ivl[,1:3])
\end{lstlisting}

Let's predict the rating user 8 would give instructor 72:\footnote{As an
exercise, the reader should check that our observed data does not
contain this rating, but that instructor 72 is in the data.}

\begin{lstlisting}
> predict(mmout.nocov,matrix(c(8,72),nrow=1))
[1] 2.912437
\end{lstlisting}

\subsection{Implementation}

Below are excerpts of the code from \textbf{trainMM()}.  First, we find
(\ref{yi.}) and (\ref{y.j}):

\begin{lstlisting}
haveCovs <- ncol(ratingsIn) > 3
n <- nrow(ratingsIn)
userRowGrps <- split(1:n, users)
itemRowGrps <- split(1:n, items)
uimean <- function(uirowgrp) mean(ratings[uirowgrp])
Yi. <- sapply(userRowGrps, uimean)
Y.j <- sapply(itemRowGrps, uimean)
\end{lstlisting}

Now for the covariates case:

\begin{lstlisting}
cmd <- paste("lmout <- lm(", nms[3], " ~ .,data=ratingsIn[,-(1:2)])",
    sep = "")
eval(parse(text = cmd))
xmeans <- function(uirowgrp) colMeans(ratingsIn[uirowgrp,
    -(1:3), drop = FALSE])
Xi. <- t(sapply(userRowGrps, xmeans))
X.j <- t(sapply(itemRowGrps, xmeans))
predsa <- predict(lmout, as.data.frame(Xi.))
predsb <- predict(lmout, as.data.frame(X.j))
...
alphai <- Yi. - predsa
betaj <- Y.j - predsb
\end{lstlisting}

Since we want to make use of the original variable names, we built up
the \textbf{lm()} call as a character string, then execute that string.

\section{Full Regression Models}

As will be seen shortly, it is natural to consider regression models for
the $Y_{ij}$.  To illustrate this, let's again consider the InstEval
data.

\subsection{Linear, Logistic}
\label{linlog}

In the original form of the InstEval data, i.e.\ not the one returned
from the \textbf{rectools} function \textbf{getInstEval()}, the first
two columns, which are the user and item IDs, are R factors.  Assuming
for now that we are not using covariates, we could
then make a call like

\begin{lstlisting}
lm(y ~ s + d, data=InstEval[,c(1,2,7]) 
\end{lstlisting}

Recall that R automatically converts factor predictors to dummy
variables, in this case 2972 of them for user IDs and 1128 for item IDs.
We could then use the return value in \textbf{predict()} calls.
The case of binary $Y$ and a logistic model would be handled similarly.
If we do use covariates, simply specify \textbf{data=InstEval} above.

This will lead to long run times, since we will have so many predictors,
in this case 2972+1128 = 4100 of them.  Indeed, that is why \textbf{lme4}
has a long run time.

However, the use of dummies in the linear case is essentially the same
as our Method of Moments approach earlier in this chapter.  This
suggests a better way:  Take our $Yi.$ and $Y.j$
as our new data.

For instance, here is row 51 of InstEval (for compactness,
without covariates):

\begin{lstlisting}
   s    d y 
51 8 1884 5       
\end{lstlisting}

The value of (\ref{yi.}) for user 8 is

\begin{lstlisting}
> tapply(InstEval$y,InstEval$s,mean)['8']
  8 
3.8 
\end{lstlisting}

while (\ref{y.j}) is

\begin{lstlisting}
> tapply(InstEval$y,InstEval$d,mean)['1884']
    1884 
3.141176 
\end{lstlisting}

We could then replace row 51 by

\begin{lstlisting}
   s    d    y 
51 3.8  3.14 5       
\end{lstlisting}

We then call \textbf{lm()} on the new data.  Let's call this the
``$\alpha,\beta$ Method.''

The \textbf{rectools} function \textbf{RStoReg()} performs this
transformation, including covariates, if any.

\subsection{Machine Learning}

\subsection{Creating New Covariates}

In the InstEval data, suppose some instructors do especially well in
service courses.  How might we take this into account?

In the parametric case, say \textbf{lm()} or \textbf{glm()},
the naive way would be to include interaction terms corresponding to
combinations of the \textbf{d} and \textbf{servie} columns in the data
frame.  This would mean products of the dummy variables for item ID and
the dummy for service course.  But as noted in Section \ref{linlog}, we
are already at risk of overfitting, and probably can ill afford adding
many terms of this type.

An alternative would be as follows.  In our transformed data (output of
\textbf{RStoReg()}), in addition to the columns corresponding to
estimated $\alpha$ and $\beta$, we could have a column for mean rating
in service courses.  Again consider row 51 in the data, originally

\begin{lstlisting}
   s    d y 
51 8 1884 5       
\end{lstlisting}

and transformed to

\begin{lstlisting}
   s    d    y 
51 3.8  3.14 5       
\end{lstlisting}

Now suppose instructor 1884 has an average of 4.8 in service courses.
Our new row 51 would be

\begin{lstlisting}
   s    d    v   y 
51 3.8  3.14 4.8 5       
\end{lstlisting}

If we then perform an analysis using \textbf{lm()} we are adding only
one element to the ``$\beta$'' vector -- much better than adding 1128
parameters, one for each of the products of the dummies for the
instructors and the service dummy.

Of course, various other applications of this trick would be possible.
For example, often there are students who do very well in one department
but less well in others.

One problem, though, say in the service course example, would be that
some instructors never teach service courses, and thus there would be no
value for them in the \textbf{v} column above.  One solution would be to
do subsetting:  We would divide our raw data into two sets, one for
instructors who teach service courses and the other for the rest.  We
could fit the richer model on the first subset, while not including this
aspect in the second.

\section{Example: InstEval}

Let's try these method on the InstEval data, and compare them to matrix
factorization:

\begin{lstlisting}
getInstEval()
set.seed(9999)
idxs <- sample(1:nrow(ivl),5000)
trn <- ivl[-idxs,]
tst <- ivl[idxs,]
# matrix factorization
mfout <- trainReco(trn)
preds.mf <- predict(mfout,tst[,-3])
mean(abs(preds.mf - tst[,3]),na.rm=T)  
# Method of Moments
mmout <- trainMM(trn)
preds.mm <- predict(mmout,tst[,-3])  
mean(abs(preds.mm - tst[,3]),na.rm=T)
# predict from alpha_i, beta_j, linear model
udconv <- RStoReg(ivl)
udconvout1 <- lm(y ~ .,data=udconv[-idxs,])
preds.conv1 <- predict(udconvout1,udconv[idxs,])
mean(abs(preds.conv1 - tst[,3]),na.rm=T)  
# predict from alpha_i, beta_j, NN
hdn <- c(8)
nnout <-
   nn.train(as.matrix(udconv[-idxs,-3]),udconv[-idxs,3],
   hidden=hdn,output='linear')
preds.dn <- nn.predict(nnout,as.matrix(udconv[idxs,-3]))
mean(abs(preds.dn - tst[,3]),na.rm=T)  
\end{lstlisting}

\begin{table}
\begin{center}
\vskip 0.5in
\begin{tabular}{|r|r|}
\hline
method & MAPE \\ \hline 
\hline
MF & 1.038 \\ \hline 
MM & 1.012 \\ \hline 
$\alpha, \beta ~ \textrm{lin.}$ & 0.956 \\ \hline 
$\alpha, \beta ~ \textrm{NN}$ & 1.118 \\ \hline 
\end{tabular}
\end{center}
\caption{}
\label{}
\end{table}

Of course, the usual caveats:

\begin{itemize}

\item What works well on the InstEval data may not be so good on some
other data set.

\item We should do several runs of each experiment, averaging their
results or taking their median, to remove the sampling variation caused
by the random split.

\item We only tried the default rank in MF, and presented only an
NN configuration (one hidden layer, 20 neurons) we found to work well.

\end{itemize} 
