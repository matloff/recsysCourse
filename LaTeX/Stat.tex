\chapter{Statistical Models}  
\label{chap:mixed} 

Let $Y_{ij}$ denote the rating user $i$ gives item $j$.   As before,
most of these values are unknown, and thus we wish to predict them.
That latter point suggests regression modeling, which we will do in this
chapter.  We'll use both the classical linear and logistic models, as
well as machine learning methods.

\section{Latent Factor Models} 

The most basic model here is

\begin{equation}
Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij},~ i = 1,...,m;~ j =
1,...,N_{i.}
\end{equation}

where $m$ is the number of users and $N_{i.}$ is the number of ratings
we have for user $i$.  (Define $N_{.j}$ to be the number of users rating
item $j$.) Other than $\mu$, all terms have expected value 0.  The terms
are interpreted as follows:

\begin{itemize}

\item $\mu$:  overall mean rating of all items, past, present and future

\item $\mu + \alpha_i$: overall mean rating of all items, past, present
and future, by user $i$; $\alpha_i$ then reflects a tendency for user
$i$ to give more liberal ratings ($\alpha_i >0$) or harsher ratings
($\alpha_i < 0$)

\item $\mu + \beta_j$: overall mean rating of item $j$ by all users,
past, present and future; $\beta_j$ then reflects a tendency for item
$j$ to receive more liberal ratings ($\beta_j >0$) or harsher ratings
($\beta_j < 0$)

\item $\epsilon_{ij}$ is a ``miscellaneous'' term that account for
all factors not accounted for in the model

\end{itemize} 

The $\alpha_i$ and $\beta_j$ terms are called \textit{latent} factors, a
term meaning ``hidden.''  We can't observe user, say, 29's tendency to give
high or low ratings; it's just a model for some kind of driving force in
that user's behavior.  The ``synthetic, representative'' users in
Chapter \ref{chap:svd} are also called \textit{latent} in the RS
literature.

The analogous model for logistic regression, i.e.\ binary $Y$, is

\begin{equation}
P(Y_{ij} = 1 ~|~ i,j) = 
\frac{1}{1 + \exp[-(\mu + \alpha_i + \beta_j)]}
\end{equation}

\subsection{Estimating the Latent Factors}

To predict the unobserved $Y_{ij}$, we will need to estimate $\mu$ and
the $\alpha_i$ and $\beta_j$.  Let's see how.

\subsubsection{Linear Case}

Intuitively, estimators in the linear case are simply sample analogs of
the corresponding population quantities:

\begin{equation}
\widehat{\mu} = \frac{1}{N} \sum_{i,j \textrm{ obs.}} Y_{ij}
\end{equation}

\begin{equation}
\label{hat.mualphabeta}
\widehat{\mu} + \widehat{\alpha}_i = \frac{1}{N_{i.}}
\sum_{j, \textrm{ obs.}}^{N_{i.}} Y_{ij}
\end{equation}

\begin{equation}
\widehat{\mu} + \widehat{\beta}_j = \frac{1}{N_{.j}}
\sum_{i, \textrm{ obs.}}^{N_{.j}} Y_{ij}
\end{equation}

where $N = \sum_{i} N_{i.}$ and ``obs.'' means summation over all
observed values, i.e.\ not the missing ones.  So for example 

\begin{equation}
\widehat{\alpha}_i = \frac{1}{N_{i.}} 
\sum_{j, \textrm{ obs.}}^{N_{i.}} Y_{ij}
- \frac{1}{N} \sum_{i,j \textrm{ obs.}} Y_{ij}
\end{equation}

The reader may recognize the latter two quantities as the biases from
the preceding chapter.

\subsubsection{Logistic Case}

The approach of the last section, a special case of a technique called
the \textit{Method of Moments}, would not work as well in the logistic
case.  For example, consider (\ref{hat.mualphabeta}), which would now be

\begin{equation}
\label{nonlin1}
\widehat{\mu} + \widehat{\alpha}_i = 
\frac{1}{N_{i.}} \sum_{j=1}^{N_{i.}}
\frac{1}{1 + \exp[-(\widehat{\mu} + \widehat{\alpha_i} + \widehat{\beta_j})]}
\end{equation}

Now we'd have a set of nonlinear equations to solve.

Instead, what people use is the \textit{Method of Maximum Likelihood}.
To be sure, we still have a set of nonlinear equations to solve, but MLE
has certain optimal statistical properties, and the \textbf{lme4}
package (included in \textbf{rectools}) uses MLE, so we'll go that
route.\footnote{The \textbf{mle} package is for a general class of
methods known as \textit{mixed models}, not specifically for RS.}

To see how MLE works, consider the following game.  I toss a coin until
I accumulate $r$ heads.  I don't tell you what value I've chosen for
$r$, but I do tell you $K$, the number of tosses I needed.  You then
must guess the value of $r$.  Well, elementary probability calculations 
show that\footnote{This is a family of distributions known as
\textit{negative binomial.}}

\begin{equation}
\label{negbin}
P(K = u) = \binom{u-1}{r-1} 0.5^u,~ u = r, r+1, ...
\end{equation}

Say I tell you $K = 7$.  Then what you might do is find the value of $r$
that maximizes 

\begin{equation}
\label{negbin7}
\binom{6}{r-1} 0.5^5
\end{equation}

You are asking, ``What value of $r$ would have made our data ($K = 5$)
most likely?''  Trying $r = 1,2,...,5$, one finds that $r = 4$ maximizes
(\ref{negbin7}), so we would take that as our guess.

Now back to the RS estimation.  It is assumed that the $\alpha_i$ and
$\beta_j$ are independent, normally distributed random variables.
The joint likelihood of all the observed $Y_{ij}$ is then quite complex,
but it can be maximized, which is what \textbf{lme4} does for
\textbf{rectools}.  

\subsection{Incorporation of Covariate Data}

Instead of the old model, 

\begin{equation}
Y = \mu + \alpha + \beta + \epsilon
\end{equation}

we now have

\begin{equation}
Y = X' \gamma + \alpha + \beta + \epsilon
\end{equation}

where $X$ is the vector of covariates and $\gamma$ is the vector of
linear regression coefficients.  Note that the old $\mu$ term is now
absent, at least explicitly.  It now has become $\gamma_0$, and $X$ has
a 1 component.

\subsection{Example:  InstEval Data}

\begin{lstlisting}
> getInstEval()
> mmout <- trainMM(ivl)
\end{lstlisting}

Let's browse:

\begin{lstlisting}
> names(mmout)
[1] "alphai" "betaj"  "lmout"  "Ni."    "N.j" 
> mmout$alphai[4]
     1000 
-1.095866 
> mmout$alphai[5]
    1001 
0.892603 
\end{lstlisting}

So user 1000 seems to give harsher ratings than average, while user 1001
is more liberal in her ratings.\footnote{The reader may wonder why user
1000 would show up fourth in this vector of $\widehat{\alpha}_i$.  The
reason is that the code indexes users by the character form of their
IDs.}

The above used the covariates.  Here is the no-covariates version:

\begin{lstlisting}
mmout.nocov <- trainMM(ivl[,1:3])
\end{lstlisting}

Let's predict the rating user 8 would give instructor 72:\footnote{As an
exercise, the reader should check that our observed data does not
contain this rating, but that instructor 72 is in the data.}

\begin{lstlisting}
> predict(mmout.nocov,matrix(c(8,72),nrow=1))
[1] 2.912437
\end{lstlisting}

\section{Full Regression Models}

\subsection{Linear, Logistic}

\subsection{Machine Learning}
