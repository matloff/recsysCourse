\chapter{Some Infrastructure: Probability and Statistics}
\label{chap:infra2}   

Many RS methods are probabilistic in nature, so we will lay out some
infrastructure.  It is assumed the reader has a background in
calculus-based probability structures, such as expected value and
density functions.  Background in statistics (as opposed to probability)
and machine learning is \textit{not} assumed.

\section{Data as a Sample}

In statistics, the data are usually considered a sample from a
population.  For instance, during an election campaign pollsters will
take a \underline{sample}, typically of 1200 people, from the
\underline{population} of all voters.  Say they are interested in $p$,
the population proportion of voters favoring Candidate Jones. They
calculate their \underline{estimate} of $p$, denoted $\widehat{p}$, to
be the proportion of voters in the \underline{sample} who like Jones.

Sometimes the notion of sampling is merely conceptual. Say for instance
we are studying hypertension, on data involving 1000 patients.  We think
of them as a sample from the population of all sufferers of
hypertension, even though we did not go through an actual sampling
process.

In RS contexts, this means that we treat the users in our dataset as a
sample from the conceptual population of all potential users.  We may
even treat the items as a sample from a conceptual population of all
possible items.

In machine learning circles, it is not customary to think explicitly in
terms of populations, samples and estimates.  Nevertheless, it's
implied, as ML people do talk about predicting new data from the model
fitted on the original data.  For the model to be useful, the new data
needs to come from the same source as the original --- what
statisticians call a population.

We will usually think in terms of sample data here.

\section{Probability, Expected Value and Variance}

We will speak in terms of a \textit{repeatable experiment}, which again
could be physical or conceptual.

We think of probability as the long-run proportion of the time some
event occurs.  Say we toss a fair coin.  What do we mean by
$P(\textrm{heads} = 0.5)$?  Here our repeatable experiment is tossing the
coin.  If we were to perform that experiment many times ---
ideally, infinitely many times --- then in the long run, 50\% of the
repetitions would give us heads.

Now suppose our experiment, say a game, is to keep tossing a coin until
we get three consecutive heads.  Let $X$ denote the number of tosses
needed.  Then for instance $P(X = 4) = 0.5^4 = 0.0625$ (we get a tail
then three heads).  Imagine doing this experiment infinitely many times:
We toss the coin until we get three consecutive heads, and record $X$;
we toss the coin until we get three consecutive heads, and record $X$;
we toss the coin until we get three consecutive heads, and record $X$;
and so on.  This would result in infinitely many $X$ values.  Then in
the long run, 6.25\% of the $X$ values would be 4.

The \textit{expected value} $E(X)$ of a random variable $X$ is its long-run
average value over infinitely many repetitions of the experiment.  In
that 3-consecutive heads game above, it can be shown that $E(X) = 14.7$.
In other words, if we were to play the game infinitely many times,
yielding infinitely $X$ values, their long-run average would be 14.7.

If there is no danger of ambiguity, we usually omit the parentheses,
writing $EX$ instead of $E(X)$.

The \textit{variance} of a random variable is a measure of its
dispersion, i.e.\ how much it varies from one repetition to the next.
It is defined as $Var(X) = E[(X - EX)^2]$.

Say we have a population of people and our our experiment is to 
randomly draw one person from the population, denoting that person's
height by $H$.  Then intuitively, $EH$ will be the mean height of all
the people in the population, traditionally written as $\mu$.

\section{Regression Models} 

These models, both \textit{parametric} and \textit{nonparametric},
\textbf{form the very core of statistics and machine learning (ML)}.  Their
importance cannot be overemphasized.\footnote{For many, the term
\textit{regression analysis} connotes a linear parametric model.  But
actually the term is much more general, defined to be the conditional
mean as discussed below.  Most ML techniques are nonparametric, as
explained below, but are still regression methods.}

\subsection{Definition}

Suppose we are predicting a variable $Y$ from a vector $X$ of variables,
say predicting human weight from the vector (height,age).  The
\textit{regression function} at $t = (t_1,t_2)$ of $Y$ on $X$ is defined
to be the mean weight of all people in the subpopulation consisting of
all people of height $t_1$ and age $t_2$.

Let $W$, $H$ and $A$ denote weight, height and age.  We write the
regression function as the \textit{conditional expectation} of $W$ given
$H$ and $A$, 

\begin{equation}
\label{regdef}
E(W ~| H=t_1, A=t_2)
\end{equation}

If, say $E(W ~| H=70, A=28) = 162$, it means that the mean weight of all
people in the subpopulation consisting of 28-year-olds of height 70 is
162.

Note that in (\ref{regdef}), the expression has a different value for
each $(t_1,t_2)$ pair.  So it is a \underline{function} of $t_1$ and
$t_2$.  This is called the \textit{regression function} of $W$ on $H$
and $A$.

\subsection{Prediction}

Say we have a person whose height and age are 70 and 28, but with
unknown weight.  It can be shown that the optimal (under a certain
criterion) predictor of her weight is $E(W ~| H=70, A=28) = 162$.

\subsection{Estimation}

The regression function is an attribute of the population.  Yet all we
have is sample data.  How do we estimate the regression function from
our data?

\subsubsection{Nonparametric}

Intuitively, we could use a nearest-neighbor approach.  To estimate
$E(W ~| H=70, A=28)$, we could find, say, the 25 people in our
sample for whom $(H,A)$ is closest to (70,28), and average their weights
to produce our estimate of $E(W ~| H=70, A=28)$.  

This kind of approach is common in ML.  The famous \textit{random
forests} method is basically a more complex form of kNN, as we will see
in Chapter \ref{chap:covars}.  

Statisticians also use methods like kNN.  In fact, kNN and random
forests were invented by statisticians.  But more commonly, statistics
uses \textit{parametric} methods, as follows.

\subsubsection{Parametric}

The basic idea is to assume the regression function is linear in
\underline{parameters} $\beta_i$, e.g.

\begin{equation}
\label{wthtage}
\textrm{mean weight} = \beta_0 + \beta_1 ~ \textrm{height} + \beta_2 ~ \textrm{age}
\end{equation}

for some unknown values of the $\beta_i$.

Make sure to take careful note of the word ``mean''!  Clearly, the
weights of \underline{individual} people are not linear functions of
their height and age.

As noted, the $\beta_i$ are unknown, and need to be estimated
from our sample data.  The estimates will be denoted
$\widehat{\beta}_i$.  They are obtained by minimizing a certain sum of
squares, to be discussed in Section \ref{lmdetails}.

By the way, if the reader is familiar with
\textit{neural networks}, she may be surprised that this technique is
also parametric.  Again, more in Chapter \ref{chap:covars}.

\subsubsection{Comparison}

Consider (\ref{wthtage}), our model for the function of $t_1$ and $t_2$

\begin{equation}
E(\textrm{weight} ~|~ \textrm{height} = t_1, \textrm{age} = t_2)
\end{equation}


With the linear assumption (\ref{wthtage}), we will be estimating three
quantities, the $\beta_i$.  But with a nonparametric approach, we are
estimating infinitely many quantities, one for each value of the
$(t_1,t_2)$ pair.

In other words, \textbf{parametric methods are a form of dimension reduction}.
On the other hand, this reduction comes at the expense of relying on the
assumption of linearity in (\ref{wthtage}).  However, this is not so
restrictive as it may seem, because:

\begin{itemize}

\item There are ways to assess the validity of the assumption.  This is
covered in almost any book on regression, such as mine
(N. Matloff, \textit{Statistical Regression and Classification: from
Linear Models to Machine Learning}, CRC, 2017.

\item One can add polynomial terms, as seen in the next section.

\item Assumptions tend to be less important in prediction contexts than
in estimation.  In the RS context, for instance, a rough model may be
fine if we wish to take into account a gender effect on ratings, but
might be insufficient if we want to estimate the actual magnitude of
gender effect.

\end{itemize} 

\subsection{The lm() Function in R}

In R, the workhorse regression estimator is the \textbf{lm()} function.
Let's apply this to the MovieLens data, predicting rating from age and
gender.  We'll define the latter as 1 for male, 0 for female.  We find
that our estimated regression function of rating on age and gender is

\begin{equation}
\textrm{estimated mean rating} = 3.3599 + 0.005311 ~ \textrm{age} 
- 0.0069 ~ \textrm{gender}
\end{equation}

Actually, this shows that age and gender are pretty weak predictors of
movie rating, which you will recall is on a scale of 1 to 5.  A 10-year
difference in age raised the predicted rating only by about 0.05!  The
effect of gender is small too.  And while it is interesting to see that
older people tend to give slighly higher ratings, as do women, we must
keep in mind that the magnitude of the effect here is
small.\footnote{You may be familiar with the term \textit{statistically
significant}.  It is generally recognized today that this term can be
quite misleading.  This is beyond the scope of this book, but suffice it
to say that although age and gender are statistically significant above
(details in the R output), their practical importance as predictors here
is essentially nil.  See R. Wasserstein and N. Lazar, The ASA's
Statement on p-Values: Context, Process, and Purpose, \textit{The
American Statistician}, June 2016.}

Here is the annotated code:

\begin{lstlisting}

# read (user,item,rating,transID) data; name the columns
ratings <- read.table('u.data') 
names(ratings) <- c('usernum','movienum','rating','transID') 
# read demographic data
demog <- read.table('u.user',sep='|') 
names(demog) <- c('usernum','age','gender','occ','ZIP') 
# merge (database 'join' op)
u.big <- merge(ratings,demog,by.x=1,by.y=1) 
u <- u.big[,c(1,2,3,5,6)] 
# fit linear model
lmout <- lm(rating ~ age+gender,data=u) 

\end{lstlisting}

Here's the output:

\begin{lstlisting}
> lmout

Call:
lm(formula = rating ~ age + gender, data = u)

Coefficients:
(Intercept)          age      genderM  
   3.359894     0.005311    -0.006904  
\end{lstlisting}

\subsection{Details of Linear Regression Estimation}
\label{lmdetails}

In the weight-height-age example, say, we form 

\begin{equation}
\label{rss}
r =
\sum_{i=1}^n [W_i - (b_0 + b_1 H_i + b_2 A_i)]^2
\end{equation}

where $W_i$ is the weight of the i$^{th}$ person in our sample data and
so on. This is the sum of squared prediction errors.  We take derivatives
with respect to the $b_k$ to minimize, then set $\widehat{\beta}_k$ to
the minimizing $b_k$.  

Though R will do the minimizing for us, it is worth having an idea how
it works, especially as more practice in following matrix-centric
derivations.  To get a glimpse of this, we look at a matrix formulation, as
follows.  Let $A$ denote the matrix of predictor values, with a 1s
column tacked on at the left.  In the above example, row 12, say, of $A$
would consist of a 1, followed by the height and age of the 12$^{th}$
person in our sample.  Let $D$ denote the vector of weights, so that
$D_{12}$ is the weight of the 12$^{th}$ person in our sample.  Finally,
let $b$ denote the vector of $b_k$.  Then

\begin{equation}
\label{}
r = (D - A b)' (D - Ab)
\end{equation}

(Write it out to see this.)

Write the \textit{gradient} of $r$ with respect to $b$,

\begin{equation}
\frac{\partial r}{\partial b} = (
\frac{\partial r}{\partial b_0},
\frac{\partial r}{\partial b_1},
...
\frac{\partial r}{\partial b_p}
)'
\end{equation}

where $p$ is the number of predictor variables.

It can be shown that for a vector $u$,

\begin{equation}
\frac{\partial u'u}{\partial b} = 2u
\end{equation}

(analogous to the scalar relations $d (u^2) /du = 2u$; again, this is
seen by writing the expressions out).

Setting $u = D - Ab$ and applying the Chain Rule (adapted for
gradients), we get

\begin{equation}
\frac{\partial r}{\partial b} = 2 (-A') (D - Ab) 
\end{equation}

The the gradient to 0 and solving for $b$, we have

\begin{equation}
0 = A'D - A'Ab
\end{equation}

so that the minimizing $b$ is

\begin{equation}
\label{famouslm}
b = (A'A)^{-1} A'D
\end{equation}

This is what \textbf{lm()} computes in finding the $\widehat{\beta}_k$.

\subsubsection{Polynomial Terms}

People tend to gain weight during middle age, but often they lose weight
when they become elderly.  So (\ref{wthtage}), which is linear in the
age variable, may be rather unrealistic; we might believe a quadratic
model for mean weight as a function of age is better:

\begin{equation}
\label{wthtage2}
\textrm{mean weight} = \beta_0 + \beta_1 ~ \textrm{height} + 
\beta_2 ~ \textrm{age} +
\beta_3 ~ \textrm{age}^2
\end{equation}

A key point is that this is still a linear model! When we speak of a
linear model --- the 'l' in ``lm()'' -- we mean linear in the $\beta_i$.
If in (\ref{wthtage2}) we, say, multiply all the $\beta_i$ by 3, the
entire sum grows by a factor of 3.

\subsection{Categorical Variables (Predictor, Response)}

A \textit{categorical} variable is one that codes categories.  In our RS
context, for instance, a user's postal code --- ZIP Code, in the US ---
may be a helpful predictor of a rating.  Yet it cannot be treated in
\textbf{lm()}, say as a numeric variable.  After all, the ZIP Code
90024, for example, is not ``twice as good'' as 45002; they are just ID
codes.

\subsubsection{Dummy Variables as Predictors}

So, what do we do if we wish to use a categorical variable as a
predictor?  The answer is that we break the variable into a set of
\textit{indicator variables}, colloquially known as \textit{dummy
variables}.  These have the values 1 and 0, with 1
\underline{indicating} the trait in question, 0 meaning not.

Say for instance in RS we have the categorical variable State for users
in a US state.  We would define 50 dummy variables, one for each state.
For instance, the one for California would have the value 1 if the user
lives in California, 0 otherwise.

Note carefully though that we would only use 49 of the dummies, not 50.
We could for instance exclude Wyoming.  Why? Because if the other 49
dummies are all 0, then we know this user must be in Wyoming.  The
Wyoming dummy would then be redundant. Not only do we want to avoid
redundancy on dimension reduction grounds, but also that redundancy
would result in the matrix $A$ in (\ref{famouslm}) being less than full
rank, so $(A'A)^{-1}$ would not exist.

\textit{Categorical variables in R:}

In R, categorical variables are stored as objects of the class
\textbf{"factor"}.  In our MovieLens example above, let's take a look at
the data frame \textbf{demog}:

\begin{lstlisting}
> for (i in 1:5) 
+    print(class(demog[,i]))
[1] "integer"
[1] "integer"
[1] "factor"
[1] "factor"
[1] "factor"
\end{lstlisting}

The last three columns are factors.\footnote{Even the first two could
have been stored as factors, but were not coded as so.}  Let's see how
many occupations there are:

\begin{lstlisting}
> levels(demog$occ)
 [1] "administrator" "artist"        "doctor"        "educator"     
 [5] "engineer"      "entertainment" "executive"     "healthcare"   
 [9] "homemaker"     "lawyer"        "librarian"     "marketing"    
[13] "none"          "other"         "programmer"    "retired"      
[17] "salesman"      "scientist"     "student"       "technician"   
[21] "writer"       
\end{lstlisting}

In a regresison application, we'd form 21 dummies, but use only 20 of
them.

However, the designers of R (and its predecessor S), in their wisdom,
set things up to save us some time.  We can just specify the factors we
wish to use, and R will form the dummies for us, being careful to drop
one of them.

This is what happened, for instance, in our example above in which we
regressed rating against age and gender, with output

\begin{lstlisting}
Coefficients:
(Intercept)          age      genderM  
   3.359894     0.005311    -0.006904  
\end{lstlisting}

R noticed that gender was an R factor, with levels M and F.  It created
dummies for M and F, but just retained the former, as the first case in
the data had gender as M.

So, the estimated coefficient -0.0076904 meant that the ``maleness''
impact on mean rating was that value.  So, men give slightly lower
ratings than women do, for fixed age.

\subsubsection{Dummy Variables as Response Variables}


\section{Bias, Variance, Overfitting and p-Hacking}

By far the most vexing issue in statistics and machine learning is that
of \textit{overfitting}.

