\chapter{Some Infrastructure}
\label{chap:infra} 

There are some issues that will come up frequently.  We'll treat them in
detail later, but give overviews here.

\section{Data as a Sample}

In statistics, the data are usually considered to be a sample from a
population.  For instance, during an election campaign pollsters will
take a \underline{sample}, typically of 1200 people, from the
\underline{population} of all voters.  Say they are interested in $p$,
the population proportion of voters favoring Candidate Jones. They
calculate Their \underline{estimate} of $p$, denoted $\widehat{p}$, to
be the proportion of voters in the \underline{sample} who like Jones.

Sometimes the notion of sampling is merely conceptual. Say for instance
we are studying hypertension, on data involving 1000 patients.  We think
of them as a sample from the population of all sufferers of
hypertension, even though we did not go through an actual sampling
process.

In machine learning circles, it is not customary to think explicitly in
terms of populations, samples and estimates.  Nevertheless, it's
implied, as ML people do talk about predicting new data from the model
fitted on the original data.  For the model to be valid, the new data
needs to come from the same source as the original --- what
statisticians call a population.

We will usually think in terms of sample data here.

\section{Probability, Expected Value and Variance}

We will speak in terms of a \textit{repeatable experiment}, which again
could be physical or conceptual.

We think of probability as the long-run proportion of the time some
event occurs.  Say we toss a fair coin.  What do we mean by
$P(\textrm{heads} = 0.5$?  Here our repeatable experiment is tossing the
coin once.  If we were to perform that experiment many times ---
ideally, infinitely many times --- then in the long run, 50\% of the
repetitions would give us heads.

Now suppose our experiment, say a game, is to keep tossing a coin until
we get three consecutive heads.  Let $X$ denote the number of tosses
needed.  Then for instance $P(X = 4) = 0.5^4 = 0.0625$ (we get a tail
then three heads).  Imagine doing this experiment infinitely many times,
resulting in infinitely many $X$ values.  Then in the long run, 6.25\%
of the $X$ values would be 4.

The \textit{expected value} $E(X($ of a random variable $X$ is its long-run
average value over infinitely many repetitions of the experiment.  In
that 3-consecutive heads game above, it can be shown that $E(X) = 14.7$.
In other words, if we were to play the game infinitely many times,
yielding infinitely $X$ values, their long-run average would be 14.7.

If there is no danger of ambiguity, we usually omit the parentheses,
writing $EX$ instead of $E(X)$.

The \textit{variance} of a random variable is a measure of its
dispersion, i.e.\ how much it varies from one repetition to the next.
It is defined as $Var(X) = E[(X - EX)^2]$.

Say we have a population of people and our our experiment is to 
randomly draw one person from the population, denoting that person's
height by $H$.  Then intuitively, $EH$ will be the mean height of all
the people in the population, traditionally written as $\mu$.

\section{Regression Models} 

These models, both \textit{parametric} and \textit{nonparametric},
\textbf{form the very core of statistics and machine learning (ML)}.  Their
importance cannot be overemphasized.

\subsection{Definition}

Suppose we are predicting a variable $Y$ from a vector $X$ of variables,
say predicting human weight from the vector (height,age).  The
\textit{regression function} at $t = (t_1,t_2)$ of $Y$ on $X$ is defined
to be the mean weight of all people in the subpopulation consisting of
all people of height $t_1$ and age $t_2$.

Let $W$, $H$ and $A$ denote weight, height and age.  We write the
regression function as the \textit{conditional expectation} of $W$ given
$H$ and $A$, 

\begin{equation}
\label{regdef}
E(W ~| H=t_1, A=t_2)
\end{equation}

If, say $E(W ~| H=70, A=28) = 162$, it means that the mean weight of all
people in the subpopulation consisting of 28-year-olds of height 70 is
162.

Note that in (\ref{regdef}), the expression has a different value for
each $(t_1,t_2)$ pair.  So it is a \underline{function} of $t_1$ and
$t_2$.  This is called the \textit{regression function} of $W$ on $H$
and $A$.

\subsection{Prediction}

Say we have a person whose height and age are 70 and 28, but with
unknown weight.  It can be shown that the optimal (under a certain
criterion) predictor of her weight is $E(W ~| H=70, A=28) = 162$.

\subsection{Estmation}

The regression function is an attribute of the population.  Yet all we
have is sample data.  How do we estimate the regression function from
our data?

Intuitively, we could use a nearest-neighbor approach.  To estimate
$E(W ~| H=70, A=28) = 162$, we could find, say, the 25 people in our
sample for whom $(H,A)$ is closest to (70,28), and average their weights
to produce our estimate of $E(W ~| H=70, A=28) = 162$.  

This kind of approach is common in ML.  The famous \textit{random
forests} method is basically a more complex form of kNN.

Statisticians also use methods like kNN.  In fact, kNN and random
forests were invented by statisticians.  But more commonly, statistics
uses \textit{parametric} methods, as follows.

The basic idea is to assume the regression function is linear in
parameters $\beta_i$, e.g.

mean weight = $\beta_0$ + $\beta_1$ height + $\beta_2$ age

\section{Bias, Variance and Overfitting}

By far the most vexing issue in statistics and machine learning is that
of \textit{overfitting}.

\section{Partitioned Matrices}

